{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e447843",
   "metadata": {},
   "source": [
    "# Lab 6: Non-linear Classifiers (Part 2: ANNs)\n",
    "CSE2510 Machine Learning 2023/2024  \n",
    "\n",
    "*Originally developed for TI3145TU Machine Learning and Introduction to AI*  \n",
    "*Revised for CSE2510 Machine Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c8756",
   "metadata": {},
   "source": [
    "* **What?** This nonmandatory lab consists of several programming tasks and pen-and-paper questions. \n",
    "\n",
    "* **Why?** The exercises are meant to help you learn about the concepts of neural networks.\n",
    "\n",
    "* **How?** Follow the exercises in the notebook on your own or with a friend. For questions and feedback please consult the TAs during the lab session.\n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c97c96",
   "metadata": {},
   "source": [
    "$\\ex{1}$ During the lecture you learned about the building block of neural networks - the perceptron algorithm. Although there have been many developments in the field since its invention in 1958, almost all new forms of neural networks are based on the same idea of interconnected perceptrons. Of course, such perceptrons are limited because they are a linear classifier while many of our classification tasks may not be linearly separable. Nevertheless, it is still very important to understand how a single perceptron works before we move on to neural networks.\n",
    "\n",
    "![Image of a perceptron](images/perceptron_1.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0222de",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{1.1}$ Above you can see a perceptron with two inputs and one bias. It uses the Heaviside step function ($H(x) = 1$ if $x > 0$, else $0$) as the activation function. Compute $z$ (weighted sum of the inputs) and $y$ output for the given inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00382ab6",
   "metadata": {},
   "source": [
    "|         x1        |         x2        |     z     |     y     |\n",
    "|-------------------|-------------------|-----------|-----------|\n",
    "|         -1        |         -1        |           |           |\n",
    "|         -1        |          1        |           |           |\n",
    "|          1        |         -1        |           |           |\n",
    "|          1        |          1        |           |           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee80f4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\">\n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfd7cb",
   "metadata": {},
   "source": [
    "Let's create a set of four samples describing the XOR problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564efa3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "ys = np.array([1, 0, 0, 1])\n",
    "\n",
    "plt.scatter(xs[:, 0], xs[:, 1], c=ys)\n",
    "plt.title(\"XOR problem\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb5bbec",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\"> \n",
    "    \n",
    "$\\q{1.2}$ Can the pattern from above be learned by a single perceptron? Why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8429b1b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\">\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c5319",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\"> \n",
    "\n",
    "$\\q{1.3}$ Can the same pattern be learned by a two-layer perceptron? Give an informal argument why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e19ee7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\">\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4d8c4",
   "metadata": {},
   "source": [
    "$\\ex{2}$ A multilayer perceptron is an example of a neural network - a set of perceptrons organized into layers where the outputs of layer $n$ feed into the layer $n + 1$. These are extremely performant classifiers which find their use in many different classification tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda5dbc5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{2.1}$ Explain in 2-3 sentences the idea of backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc7106",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\">\n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdfb1a1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{2.2}$ Why do we need to apply backpropagation to train large neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4988fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\">\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002b75f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{2.3}$ What is a forward pass in neural networks? What do we compute during the forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32ee22",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\">\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078ad16",
   "metadata": {},
   "source": [
    "$\\ex{3}$ We will now implement the multi-layer perceptron that you have seen discussed during the lecture (see below), and use it to classify the samples of the XOR problem. Our network will consist of one hidden layer with _k_ nodes. To build the MLP, we will make use of the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) from a very popular Python machine learning library, `scikit-learn`, which you will also use in the bonus assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75158dc",
   "metadata": {},
   "source": [
    "![Multi-layer perceptron](images/multilayer_perceptron.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to show the decision boundary of our model\n",
    "def plot_model(model, X):\n",
    "    h = 0.005  # step size in the mesh\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167cde5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{3.1}$ Set the values of hyper-parameters `k` and `learning_rate_init` such that our network can learn the pattern within 10 epochs. Feel free to experiment with the values (e.g. try what happens when the number of hidden neurons changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69dea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# The number of nodes in the hidden layer (index k in the diagram above)\n",
    "k = None\n",
    "# The (initial) learning rate for our multilayer perceptron\n",
    "learning_rate_init = None\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "# Our MLP classifier may have more than one hidden layer, hence sizes are given as an n-tuple\n",
    "hidden_layer_sizes = (k,)\n",
    "model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                      solver='sgd', # We will use Stochastic Gradient Descent to optimize the loss function\n",
    "                      activation = 'relu', # Rectified linear unit activation function: f(x) = max(0, x)\n",
    "                      learning_rate_init=learning_rate_init, # Learning rate: the relative weight of new observations \n",
    "                      random_state = 42)\n",
    " \n",
    "# We will train the model over 1000 epochs\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    # We use partial_fit() to update the model in a single iteration over training data\n",
    "    model.partial_fit(xs, ys, np.unique(ys))\n",
    "    # We plot the decision boundary of the model\n",
    "    plt.scatter(xs[:, 0], xs[:, 1], c=ys)\n",
    "    plt.title(f\"Epoch: {i + 1}, training set accuracy: {model.score(xs, ys)}\")\n",
    "    plot_model(model, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d8dc1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{3.2}$ Consider how the shape of the decision boundary changes between the iterations. Would you say it is easy to predict how the decision boundary of an (arbitrary) MLP model will evolve over time? Why is that the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15bc4b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\"> \n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9a429",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{3.3}$ How do the changes in the shape of the decision boundary of an MLP model compare to what we would expect from a Decision Tree? Which model tends to behave more predictably?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42100198",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\"> \n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8531006",
   "metadata": {},
   "source": [
    "$\\ex{4}$ Although Scikit-learn offers the possibility to train artificial neural networks, these remain rather rudimentary. Instead, we will use [**Keras**](https://keras.io/getting_started/), part of Google's TensorFlow library for machine learning to train neural networks. In this exercise we will go through the steps of creating a neural network with Keras that will allow us to classify images of clothing. To that end, we will load the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which is already split into a training set of 60000 images and a test set of 10000 images. Each sample corresponds to a 28 by 28 pixel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356bdaa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802976d7",
   "metadata": {},
   "source": [
    "As we have not worked with Fashion-MNIST in previous labs, it is useful to see what the various images look like. For this, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ce8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n",
    "          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
    "\n",
    "fig, axs = plt.subplots(10, 5, figsize=(12, 12))\n",
    "axs = axs.flatten()\n",
    "for index, (image, ax) in enumerate(zip(X_train_full, axs)):\n",
    "    ax.imshow(image, cmap='gray', interpolation=\"nearest\")\n",
    "    ax.set_title(labels[y_train_full[index]])\n",
    "    ax.axis('off')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237325ee",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{4.1}$ The values of the features are in range from $0$ to $255$, we would like to have them mapped to the range from $0$ to $1$. Convert all images from the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "assert np.isclose(np.amin(np.amin(X_train_full)), 0.0)\n",
    "assert np.isclose(np.amax(np.amax(X_train_full)), 1.0)\n",
    "assert np.isclose(np.amin(np.amin(X_test)), 0.0)\n",
    "assert np.isclose(np.amax(np.amax(X_test)), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e388d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{4.2}$ Before we start to develop a model, we would also like to use 10% of the training set as a validation set. Use the function [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn to generate a training set of 90% and validation set of 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 42\n",
    "X_train, X_validation, y_train, y_validation = None, None, None, None\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "assert X_train.shape == (54000, 28, 28)\n",
    "assert X_validation.shape == (6000, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece0836",
   "metadata": {},
   "source": [
    "Although Keras allows for the creation of arbitrarily complex neural networks we will stick to a simple example of a so-called Fully Connected Network. We can define our neural network as a [`Sequential`](https://keras.io/api/models/sequential/) model (representing a sequence of layers one after the other) where each layer takes one array as input and outputs another array. Such a model will start with an input layer followed by a number of hidden layers, and finally an output layer. For our purposes all layers will be `Dense` which means that every single neuron in layer $k$ is connected to every single neurons in layer $k + 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4eddd3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{4.3}$ Inspect and finalize the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape is a list of numbers corresponding to the shape of a sample, i.e. the shape of a single image\n",
    "input_shape = [None, None]\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "# Number of neurons in the hidden layer(s) is a hyper-parameter which needs to be optimized\n",
    "# We can guess that the number required here would be several dozen, maybe several hundred neurons\n",
    "neurons_1 = None\n",
    "neurons_2 = None\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "# Number of neurons in the output layer corresponds to the number of classes\n",
    "output_neurons = 0\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        # Flatten layer converts the input into a 1-dimensional array\n",
    "        keras.layers.Flatten(input_shape=input_shape),\n",
    "        # Dense layers allow for several different activation functions, we will use ReLU which is a popular choice\n",
    "        keras.layers.Dense(neurons_1, activation=\"relu\"),\n",
    "        # Each Dense layer can also receive regularizers for the bias, weights, and output\n",
    "        keras.layers.Dense(neurons_2, activation=\"relu\"),\n",
    "        # Output layers commonly use the softmax function as activation but other options are also possible\n",
    "        keras.layers.Dense(output_neurons, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# We can get the summary of the model which includes the number of parameters that require optimization in training\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415d112",
   "metadata": {},
   "source": [
    "The number of parameters for each dense layer above depends on the number of biases and connections between the neurons. Of course, very complex neural networks may have billions of parameters but ours shouldn't have more than 300-600 thousand parameters. By default, all parameters are trainable (hence `Non-trainable params: 0`) however in some cases we may want to keep a selection of parameters to constant values, which Keras also allows.\n",
    "\n",
    "Before we can train a model, we need to compile it. The `compile` function accepts various parameters but we specify 3 of them:\n",
    "* `loss` is the metric that should be minimized during training, we will use [Sparse Categorical Cross-entropy](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class) but you don't need to understand how it works\n",
    "* `optimizer` is a technique of changing the weights of a model to minimize `loss`\n",
    "* `metrics` describe performance of a model but are not directly optimized in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634c94f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{4.4}$ Finally, run the code below to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# validation_data is a tuple containing the feature values and labels\n",
    "validation_data = (X_validation, y_validation)\n",
    "\n",
    "# In case the code takes more than 2-3 minutes, consider lowering the number of epochs\n",
    "history = model.fit(X_train, y_train, epochs=15, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1025403",
   "metadata": {},
   "source": [
    "When our model is trained, we can inspect it to learn a lot of useful information. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[2].get_weights()\n",
    "print(weights.shape, biases.shape)\n",
    "weights[0, :10], biases[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830b931",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{4.5}$ What do the arrays above represent? Why are they shaped like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08098f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\"> \n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60610459",
   "metadata": {},
   "source": [
    "We can also learn about the training process by plotting the loss on the training set and validation set against the number of epochs. This data is directly available in the `history` variable which is generated during the training of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss during training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338b44e",
   "metadata": {},
   "source": [
    "Similarly, we may want to know how the accuracy of our model changed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy during training')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['training set', 'validation set'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6f6f9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "    \n",
    "$\\q{4.6}$ Does this model seem converged?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25cd62",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\"> \n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57520f03",
   "metadata": {},
   "source": [
    "Finally, we can issue predictions on the test set objects. Keras calculates the `loss` and performance `metric` as specified when the model was compiled and returns it as an array of two numbers. In our case that's `sparse_categorical_crossentropy` and `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = np.round(model.evaluate(X_test, y_test), 4)\n",
    "print(f\"Test set loss: {performance[0]}, test set accuracy: {performance[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fce49",
   "metadata": {},
   "source": [
    "$\\ex{5}$ As a demonstration of the capabilities of (deep) neural networks in the domain of image recognition, we will  take a moment to take a look at [ResNet50](https://keras.io/api/applications/resnet/#resnet50-function) which is also available via Keras (which offers [many other pretrained networks](https://keras.io/api/applications/)).\n",
    "\n",
    "Many advanced neural networks that classify images use [convolutions](https://www.ibm.com/topics/convolutional-neural-networks) as building blocks. ResNet50 is a neural network that consists of 50 convolutional layers. It was trained on the foundational ImageNet dataset of over 1 million training images representing 1000 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77529f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{5.1}$ What do you think is the biggest challenge when training neural networks from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5dd9e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\"> \n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005fc16",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "$\\q{5.2}$ What is a good strategy to avoid this issue?  \n",
    "**Hint:** take a look at the link for ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0cf55",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "<div style=\"background-color:#f1be3e\"> \n",
    "\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731de73e",
   "metadata": {},
   "source": [
    "We will see if ResNet can deal with the classification of the following images (but feel free to also try it with your own selection images later!):\n",
    "\n",
    "<img src=\"images/polar_bear.jpg\" alt=\"Polar bear\" width=\"200\"/>\n",
    "<img src=\"images/watermelon.jpg\" alt=\"Watermelon\" width=\"200\"/>\n",
    "<img src=\"images/tulip.jpg\" alt=\"Tulip flower\" width=\"200\"/>\n",
    "\n",
    "Let's load the neural network from the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680bc46d",
   "metadata": {},
   "source": [
    "Here weights indicates that we want to use the neural network weights that are the result of training on ImageNet. Because of this, we can directly use ResNet50 for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = image.load_img('images/polar_bear.jpg', target_size=(224, 224))\n",
    "polar_bear = image.img_to_array(img_1)\n",
    "polar_bear = np.expand_dims(polar_bear, axis=0)\n",
    "polar_bear = preprocess_input(polar_bear)\n",
    "\n",
    "img_2 = image.load_img('images/watermelon.jpg', target_size=(224, 224))\n",
    "watermelon = image.img_to_array(img_2)\n",
    "watermelon = np.expand_dims(watermelon, axis=0)\n",
    "watermelon = preprocess_input(watermelon)\n",
    "\n",
    "img_3 = image.load_img('images/tulip.jpg', target_size=(224, 224))\n",
    "tulip = image.img_to_array(img_3)\n",
    "tulip = np.expand_dims(tulip, axis=0)\n",
    "tulip = preprocess_input(tulip)\n",
    "\n",
    "preds_1 = model.predict(polar_bear)\n",
    "preds_2 = model.predict(watermelon)\n",
    "preds_3 = model.predict(tulip)\n",
    "\n",
    "print('Predicted for polar bear:', decode_predictions(preds_1, top=3)[0])\n",
    "print('Predicted for watermelon:', decode_predictions(preds_2, top=3)[0])\n",
    "print('Predicted for tulip:', decode_predictions(preds_3, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4728821",
   "metadata": {},
   "source": [
    "As you can see, ResNet has done really well with the polar bear image (prediction of `ice_bear` with probability of 99.9%). For the image representing a watermelon, it has correctly identified it as fruit (an all three top predictions are in fact fruits), however, it incorrectly assumed that the image represented a `fig` (probability of 83.8%). \n",
    "\n",
    "In the last case, ResNet was completely off identifying our flower as a type of yellow butterfly. Nevertheless, the assigned probability is rather low so it seems that the network wasn't very certain of its own classification either way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2239a17",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\"> \n",
    "    \n",
    "$\\q{5.3}$ Why do you think ResNet wasn't able to identify the flower at all?  \n",
    "**Hint:** You can inspect the full list of the classes of ResNet [here](https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ff0a2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\">\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "_Write your answer here._\n\n",
    "[//]: # (END ANSWER)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
